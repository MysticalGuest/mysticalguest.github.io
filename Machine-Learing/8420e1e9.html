<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="generator" content="Hexo 4.2.1"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="baidu-site-verification" content="code-kyGo8s2mM1"><title>机器学习导论 - Longing(MysteryGuest的博客)</title><meta description="Introduction to Machine Learning"><meta property="og:type" content="blog"><meta property="og:title" content="机器学习导论"><meta property="og:url" content="https://mysticalguest.github.io/"><meta property="og:site_name" content="Longing(MysteryGuest的博客)"><meta property="og:description" content="Introduction to Machine Learning"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://gitee.com/mysteryguest/ObjectStorage/raw/master/MachineLearing/woman.jpg"><meta property="article:published_time" content="2021-09-08T00:57:29.775Z"><meta property="article:modified_time" content="2021-11-09T09:05:42.888Z"><meta property="article:author" content="MysticalGuest"><meta property="article:tag" content="算法"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="https://gitee.com/mysteryguest/ObjectStorage/raw/master/MachineLearing/woman.jpg"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://mysticalguest.github.io/Machine-Learing/8420e1e9.html"},"headline":"MysteryGuest","image":["https://gitee.com/mysteryguest/ObjectStorage/raw/master/MachineLearing/woman.jpg"],"datePublished":"2021-09-08T00:57:29.775Z","dateModified":"2021-11-09T09:05:42.888Z","author":{"@type":"Person","name":"MysticalGuest"},"description":"java,技术分享,后端开发,算法,spring"}</script><link rel="canonical" href="https://mysticalguest.github.io/Machine-Learing/8420e1e9.html"><link rel="alternative" href="/atom.xml" title="Longing(MysteryGuest的博客)" type="application/atom+xml"><link rel="icon" href="/img/bird.ico"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/font-awesome/5.12.0/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.loli.net/css2?family=Oxanium:wght@300;400;600&amp;family=Roboto+Mono"><link rel="stylesheet" href="/css/cyberpunk.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><script src="/js/click.js"></script><script src="/js/anime.min.js"></script><canvas class="fireworks" style="position: fixed;left: 0;top: 0;z-index: 1; pointer-events: none;"></canvas><script src="/js/snow.js"></script><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/mylogo.svg" alt="Longing(MysteryGuest的博客)" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">首页</a><a class="navbar-item" href="/archives">归档</a><a class="navbar-item" href="/categories">分类</a><a class="navbar-item" href="/tags">标签</a><a class="navbar-item" href="/friend">友链</a><a class="navbar-item" href="/about">关于</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Visit on GitHub" href="https://github.com/MysticalGuest"><i class="fab fa-github"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="目录" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main"><div class="card"><div class="card-image"><span class="image is-7by3"><img class="thumbnail" src="https://gitee.com/mysteryguest/ObjectStorage/raw/master/MachineLearing/woman.jpg" alt="机器学习导论"></span></div><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" dateTime="2021-09-08T00:57:29.775Z" title="2021-09-08T00:57:29.775Z">2021-09-08</time><span class="level-item"><a class="link-muted" href="/tags/%E7%AE%97%E6%B3%95/">算法</a></span><span class="level-item"><a class="link-muted" href="/categories/Machine-Learing/">Machine-Learing</a></span><span class="level-item">1 小时 读完 (大约 9741 个字)</span><span class="level-item" id="busuanzi_container_page_pv"><i class="far fa-eye"></i>&nbsp;&nbsp;<span id="busuanzi_value_page_pv">0</span>次访问</span></div></div><h1 class="title is-3 is-size-4-mobile">机器学习导论</h1><div class="content"><p>Introduction to Machine Learning</p>
<a id="more"></a>
<h1 id="第一章-绪论"><a href="#第一章-绪论" class="headerlink" title="第一章 绪论"></a>第一章 绪论</h1><h2 id="1-1-什么是机器学习"><a href="#1-1-什么是机器学习" class="headerlink" title="1.1 什么是机器学习"></a>1.1 什么是机器学习</h2><p>Arthur Samuel定义的机器学习：在没有明确设置的情况下，使计算机具有学习能力的研究领域。</p>
<p>Tom Mitchell定义的机器学习：一个适当的学习问题定义如下：计算机程序从<strong>经验E</strong>中学习，解决某一<strong>任务T</strong>进行某一<strong>性能度量P</strong>，通过P测试在T上的表现因经验E而提高。</p>
<p>最主要的两类学习算法：监督学习和无监督学习。</p>
<ul>
<li>监督学习：我们会教计算机做某件事情；</li>
<li>无监督学习：我们让计算机自己学习；</li>
</ul>
<p>机器学习方法在大型数据库中的应用称为数据挖掘（data mining）。</p>
<p>机器学习不仅仅是数据库方面的问题，他也是人工智能的组成部分。机器学习还可以帮助我们解决视觉、语音识别以及机器人方面的许多问题。</p>
<h2 id="1-2-机器学习的应用实例"><a href="#1-2-机器学习的应用实例" class="headerlink" title="1.2 机器学习的应用实例"></a>1.2 机器学习的应用实例</h2><h3 id="1-2-1-学习关联性"><a href="#1-2-1-学习关联性" class="headerlink" title="1.2.1 学习关联性"></a>1.2.1 学习关联性</h3><p>购物篮分析：发现顾客所购商品之间的关联性；打包策略。</p>
<p>关联规则，<script type="math/tex">P(chips|beer)=0.7</script>，定义规则，购买啤酒的顾客中有<script type="math/tex">70%</script>的人也买了薯片。</p>
<h3 id="1-2-2-分类"><a href="#1-2-2-分类" class="headerlink" title="1.2.2 分类"></a>1.2.2 分类</h3><p><strong>信用评分</strong>，客户属性及其风险性的关联性。</p>
<p><strong>分类</strong>：低风险客户和高风险客户。客户信息作为分类器的输入，分类器的任务是<strong>将输入指派</strong>到其中的一个<strong>类</strong>。</p>
<p>规则的用途是<strong>预测</strong>。</p>
<p><strong>模式识别</strong>方面的应用：光学字符识别（OCR），即从字符图像识别字符编码。</p>
<p><strong>人脸识别</strong>输入的是人脸，类是需要识别的人，并且学习程序应当学习人脸图像与身份识别之间的关联性。这个问题比OCR更困难，原因是人脸会有更多的类，输入图像也更大一些，并且人脸是三维的，不同的姿势和光线等都会导致图像的显著变化。</p>
<p>对于<strong>医学诊断</strong>（medical diagnosis），输入是关于患者的信息，而类是疾病。输人包括患者的年龄、性别、既往病史、目前症状等。</p>
<p>在<strong>语音识别</strong>（speech recognition），输入是语音，类是可以读出的词汇。这里要学习的是从语音信号到某种语言的词汇的关联性。由于年龄、性别或口音方面的差异，不同的人对于相同词汇的读音不同，这使得语音识别问题相当困难。语音识别的另一个特点是其输入信号是时态的（temporal），词汇作为音素的序列实时读出，而且有些词汇的读音会较长一些。一种语音识别的新方法涉及利用照相机记录口唇动作，作为语音识别的补充信息源。这需要<strong>传感器融合</strong>（sensor fusion）技术，集成来自不同形态的输入，即集成声音和视频信号。</p>
<blockquote>
<p>机器学习，自然语言处理，垃圾邮件过滤，机器翻译</p>
</blockquote>
<p>生物测定学（biometrics）使用人的生理和行为特征来识别或认证人的身份，需要集成来自不同形态的输人。生理特征的例子是面部图像、指纹、虹膜和手掌；行为特征的例子是签字的力度、嗓音、步态和击键。</p>
<p>从数据中学习规则也为<strong>知识抽取</strong>提供了可能性。风险识别。</p>
<p>机器学习还可以进行<strong>压缩</strong>。用规则拟合数据，得到比数据更简单的解释，存储空间更好，计算更少。</p>
<p><strong>离群点检测</strong>，即发现那些不遵守规则的例外实例。</p>
<h3 id="1-2-3-回归"><a href="#1-2-3-回归" class="headerlink" title="1.2.3 回归"></a>1.2.3 回归</h3><p><strong>回归（regression）问题</strong>：预测二手车价格的系统。输入是影响车价的属性信息：品牌、车龄、发动机性能等，输出是车的价格，这种输出为数值的问题。</p>
<p>调查交易情况，收集训练数据，机器学习程序用一个函数拟合这些数据来学习x的函数y。</p>
<p>回归和分类均为监督学习（supervised learning）问题，其中输入x和输出y给定，任务是学习从输入到输出的映射。参数模型，判别式函数，误差最小，非线性函数。</p>
<p>回归的另一个例子是对<strong>移动机器人</strong>的导航。例如，自动汽车导航。其中输出是每次转动车轮的角度，使得汽车前进而不会撞到障碍物或偏离车道。这种情况下，输入由汽车上的传感器（如视频相机、GPS等）提供。训练数据可以通过监视和记录驾驶员的动作收集。</p>
<p>假设造一个焙炒咖啡的机器，该机器有多个影响咖啡品质的输入：各种温度、时间、咖啡豆种类等配置。针对不同的输入配置进行大量试验，并测量测量咖啡的品质。为寻求最优配置，我们拟合一个<strong>联系</strong>这些输入和咖啡品质的回归模型，并在当前模型的最优样本附近选择一些新的点，以便寻找更好的配置。我们抽取这些点，检测咖啡的品质，将它们加入训练数据，并拟合新的模型。这通常被称为<strong>响应面设计</strong>（response surface design）。</p>
<h3 id="1-2-4-非监督学习"><a href="#1-2-4-非监督学习" class="headerlink" title="1.2.4 非监督学习"></a>1.2.4 非监督学习</h3><p>监督学习，目标是学习从输入到输出的映射关系，输出的正确值已经由指导者提供。非监督学习中却没有这样的指导者,，只有输入数据。目标是发现输入数据中的规律。输入空间存在着<strong>某种结构</strong>，使得特定的模式比其他模式<strong>更常出现</strong>，而我们希望<br>知道哪些经常发生，哪些不经常发生。在统计学中，这称为密度估计（density estimation）。</p>
<p>密度估计的一种方法是聚类（clustering），其目标是发现输入数据的簇或分组。对于拥有老客户数据的公司，客户数据包括客户的个人统计信息，及其以前与公司的交易，公司也许想知道其客户的分布，搞清楚什么类型的客户会频繁出现。这种情况下，聚类模型会将属性相似的客户分派到相同的分组，为公司提供其客户的自然分组；这称作<em>客户市场划分</em>(customer segmentation)。一旦找出了这样的分组，公司就可能做出一些决策，比如对不同分组的客户提供特别的服务和产品等；这称作客户关系管理（customer relationship management）。这样的分组也可以用于识别”离群点”，即那些不同于其他客户的客户。这可能意味着一块新的市场，公司可以进一步开发。</p>
<p>聚类的有趣应用是<strong>图像压缩</strong>。图像被量化。主色调。</p>
<p>文档聚类，相似的文档分组。</p>
<p>机器学习方法还应用于生物信息学。计算机科学在分子生物学的应用领域之一就是<strong>比对</strong>，即序列匹配。很多模板串进行匹配问题。聚类用于学习结构域。</p>
<h3 id="1-2-5-增强学习"><a href="#1-2-5-增强学习" class="headerlink" title="1.2.5 增强学习"></a>1.2.5 增强学习</h3><p>在某些应用中，系统的输出是动作的序列。在这种情况下，单个的动作并不重要，重要的是策略，即达到目标的正确动作的序列。不存在中间状态中最好动作这种概念。如果一个动作是好的策略的组成部分，那么该动作就是好的。这种情况下，机器学<br>习程序就应当能够<strong>评估策略的好坏程度</strong>，并从以往好的动作序列中学习，以便能够产生策略。这种学习方法称为<strong>增强学习</strong>（reinforcement learning）算法。</p>
<p>游戏是一个很好的例子。在游戏中，单个移动本身并不重要，正确的移动序列才是重要的。如果一个移动是一个好的游戏策略的一部分，则它就是好的。游戏是人工智能和机器学习的重要研究领域，这是因为游戏容易描述，但又很难玩好。像国际象棋，规则少量几条，但非常复杂，因为在每种状态下都有大量可行的移动，并且每局又都包含有大量的移动。一旦有了能够学习如何玩好游戏的好算法，我们也可以将这些算法用在具有更显著经济效益的领域。</p>
<p>用于在某种环境下<strong>搜寻目标位置</strong>的机器人导航是增强学习的另一个应用领域。在任何时候，机器人都能够朝着多个方向之一移动。经过多次的试运行，机器人应当学到正确的动作序列，尽可能快地从某一初始状态到达目标状态，并且不会撞到任何障碍物。致使增强学习难度增加的一个因素是系统具有不可靠和不完整的感知信息。例如，装备视频照相机的机器人就得不到完整的信息，因此该机器人总是处于部分可观测状态，并且应当将这种不确定性考虑在内。一个任务还可能需要多智能主体的并行操作，这些智能主体将相互作用并协同操作，以便完成一个共同的目标。</p>
<h1 id="第二章-监督学习"><a href="#第二章-监督学习" class="headerlink" title="第二章 监督学习"></a>第二章 监督学习</h1><h2 id="2-1-由实例学习类"><a href="#2-1-由实例学习类" class="headerlink" title="2.1 由实例学习类"></a>2.1 由实例学习类</h2><p>学习，实例，被测人，涵盖目标分类的描述，预测。类识别器的输入，训练数据绘制在二维空间上，专家谈论和分析数据，确定范围，假设类集合。训练集，经验误差，预测值，预期值，泛化问题。假设，诱导类。</p>
<h1 id="上海交大张志华机器学习"><a href="#上海交大张志华机器学习" class="headerlink" title="上海交大张志华机器学习"></a>上海交大张志华机器学习</h1><p>计算，统计，信息论，算法，控制论，最优化</p>
<p>机器学习=矩阵+优化+算法+统计</p>
<p>回归问题，数据，预测估计， </p>
<h1 id="吴恩达机器学习"><a href="#吴恩达机器学习" class="headerlink" title="吴恩达机器学习"></a>吴恩达机器学习</h1><p>网页排序，图像识别，邮件过滤，</p>
<h2 id="1-3-监督学习"><a href="#1-3-监督学习" class="headerlink" title="1.3 监督学习"></a>1.3 监督学习</h2><p>数据集，包括正确答案，给出更多正确答案，回归问题，数值的连续输出</p>
<p>分类问题，预测一个离散值的输出，可能两个以上的输出值</p>
<p>无穷多的特征，向量机</p>
<h2 id="1-4-无监督学习"><a href="#1-4-无监督学习" class="headerlink" title="1.4 无监督学习"></a>1.4 无监督学习</h2><p>数据集无任何标签，聚类算法，新闻专题，计算机集群，现在octave中建立模型，然后再迁移到其他编程语言。</p>
<h2 id="2-1-模型描述"><a href="#2-1-模型描述" class="headerlink" title="2.1 模型描述"></a>2.1 模型描述</h2><p>监督学习是如何工作的，预测房价，每个例子都有一个“正确答案”，也是回归问题的例子，回归是指我们预测一个具体的数值输出。</p>
<p>另一个监督学习常见的例子是分类问题，我们用它来预测离散值输出。做判断。</p>
<ul>
<li>M：样本输入量，训练样本的数量，样本容量。</li>
<li>x：输入值，特征</li>
<li>y：输出变量，预测的目标变量</li>
<li><script type="math/tex">(x, y)</script>表示一个训练样本</li>
<li><script type="math/tex">(x^{(i)},y^{(i)})</script>表示第i个训练样本</li>
</ul>
<p><img src="https://gitee.com/mysteryguest/ObjectStorage/raw/master/MachineLearing/hypothesis.jpg" alt="图1"></p>
<p>向算法提供房价的训练集，算法输出的<strong>假设函数</strong>，作用是：把房子的大小作为输入变量，h是一个引导从x得到y的函数。</p>
<p>决定怎么表示这个假设函数h。</p>
<p>表示假设函数：<script type="math/tex">h_\theta =\theta_0+\theta_1 x</script></p>
<p>函数的作用是预测y是关于x的线性函数，线性是学习的基础。一元线性回归模型（单变量线性回归）</p>
<h2 id="2-2-代价函数"><a href="#2-2-代价函数" class="headerlink" title="2.2 代价函数"></a>2.2 代价函数</h2><h3 id="零"><a href="#零" class="headerlink" title="零"></a>零</h3><p>弄清楚如何把最有可能的直线与我们的数据相拟合，<script type="math/tex">\theta_i</script>称为模型参数，如何选择这两个参数值<script type="math/tex">\theta_0</script>和<script type="math/tex">\theta_i</script>，不同的参数值 <script type="math/tex">\to</script> 不同的假设 <script type="math/tex">\to</script> 不同的假设函数；</p>
<p>使<script type="math/tex">h(x)</script>，输入x时，我们预测的值最接近该样本对应的y值的参数<script type="math/tex">\theta_0,\theta_1</script></p>
<p>给出标准的定义，在线性回归中，我们要解决的是一个最小化问题，所以要写出<script type="math/tex">\theta_0,\theta_1</script>的最小化，式子及其小？<script type="math/tex">h(x)</script>与y之间的差异要小，减少假设的输出与房子真实价格之间的差的平方。</p>
<p>对所有训练样本进行一个求和，对<script type="math/tex">i=1</script>到<script type="math/tex">i=M</script>的样本，将对假设进行预测得到的结果，此时输入是第i号房子的面积，将第i号对应的预测结果，减去第i号房子的实际价格所得的差的平方相加得到总和。<script type="math/tex">\sum_{i=1}^M[h_\theta(x^{(i)})-y^{(i)}]^2</script>，尽量减少这个值，也就是预测值和实际值的差的平方，误差和，或者说预测价格和实际卖出价格的差的平方。<script type="math/tex">\frac{1}{M}\sum_{i=1}^M[h_\theta(x^{(i)})-y^{(i)}]^2</script>，减少平均误差，表示关于<script type="math/tex">\theta_0</script>和<script type="math/tex">\theta_1</script>的最小化过程，找到<script type="math/tex">\theta_0</script>和<script type="math/tex">\theta_1</script>，使这个值最小，转化为目标函数。<script type="math/tex">\frac{1}{2M}\sum_{i=1}^M[h_\theta(x^{(i)})-y^{(i)}]^2</script>，加<script type="math/tex">\frac{1}{2}</script>是因为后面平方求导多出2</p>
<p>定义一个代价函数，<script type="math/tex">J(\theta_0,\theta_1)=\frac{1}{2M}\sum_{i=1}^M[h_\theta(x^{(i)})-y^{(i)}]^2</script>，对于函数<script type="math/tex">J(\theta_0,\theta_1)</script>求最小值，常用（对大多数合理），平方误差函数，平方误差代价函数，MSE（mean squared error，均方差），</p>
<p>对于回归问题，是个合理的选择，其他代价函数也可</p>
<h3 id="一"><a href="#一" class="headerlink" title="一"></a>一</h3><blockquote>
<p>Hypothesis：<script type="math/tex">h_\theta(x)=\theta_0+\theta_1x</script></p>
<p>Parameters：<script type="math/tex">\theta_0,\theta_1</script></p>
<p>Cost Function：<script type="math/tex">J(\theta_0,\theta_1)=\frac{1}{2M}\sum_{i=1}^M[h_\theta(x^{(i)})-y^{(i)}]^2</script></p>
<p>Goal：<script type="math/tex">\underbrace{minimize}_{\theta_0,\theta_1} J(\theta_0,\theta_1)</script>，二次函数？</p>
</blockquote>
<p>simplified</p>
<blockquote>
<p>Hypothesis：<script type="math/tex">h_\theta(x)=\theta_1x</script></p>
<p>Parameters：<script type="math/tex">\theta_1</script></p>
<p>Cost Function：<script type="math/tex">J(\theta_1)=\frac{1}{2M}\sum_{i=1}^M[h_\theta(x^{(i)})-y^{(i)}]^2</script></p>
<p>Goal：<script type="math/tex">\underbrace{minimize}_{\theta_1} J(\theta_1)</script></p>
</blockquote>
<p>使代价函数可视化，学习算法的优化目标</p>
<h3 id="二"><a href="#二" class="headerlink" title="二"></a>二</h3><p>代价函数的作用，等高线图，等高图像，代价函数图形化，加入<script type="math/tex">\theta_0,\theta_1</script>，碗状3维图，碗状曲面，代价函数的形状，从<script type="math/tex">\theta_0,\theta_1</script>所在平面截取，即得到等高线图，以后更复杂，更高维</p>
<h2 id="2-5-梯度下降"><a href="#2-5-梯度下降" class="headerlink" title="2.5 梯度下降"></a>2.5 梯度下降</h2><p>一种算法，梯度下降法（Gradient descent algorithm），常用，可以将代价函数J最小化，应用于线性回归及机器学习的众多领域，最小化其他函数</p>
<blockquote>
<p>Have some fuction <script type="math/tex">J(\theta_0,\theta_1)</script></p>
<p>Want <script type="math/tex">\underbrace{min}_{\theta_0,\theta_1} J(\theta_0,\theta_1)</script></p>
<p>Outline：</p>
<ul>
<li>Start with some <script type="math/tex">\theta_0,\theta_1</script>（将其初始化都设为0，<script type="math/tex">\theta_0=0,\theta_1=0</script>）有时也会初始化为其他值</li>
<li>Keep changing <script type="math/tex">\theta_0,\theta_1</script> reduce <script type="math/tex">J(\theta_0,\theta_1)</script> until we hopefully end up at a minimun（最小值，局部最小值）</li>
</ul>
<p>更一般化的问题，<script type="math/tex">\theta_0,\theta_1,...,\theta_n</script></p>
</blockquote>
<p><img src="https://gitee.com/mysteryguest/ObjectStorage/raw/master/MachineLearing/JFunction.png" alt="图2"></p>
<p>从红色高峰尽快走下山，周围方向，直到收敛至局部最低点</p>
<p>算法特点：不同出发点，到达不同局部最优处</p>
<p>背后的数学原理：（梯度算法的定义）</p>
<blockquote>
<p>repeat until convergence（收敛）{</p>
<p>​        <script type="math/tex">\theta_j:=\theta_j-\alpha\frac{\partial}{\partial\theta_j}J(\theta_0,\theta_1)\ (for\ j=0\ and\ j=1)</script></p>
<p>}    :=表示赋值，a=b真值判定，第一部分，<script type="math/tex">\alpha</script>是被称为学习速率的数字，用来控制梯度下降时，我们迈出多大的步子，<script type="math/tex">\alpha</script>越大，梯度下降很迅速，它控制我们以多大的幅度更新这个参数<script type="math/tex">\theta_j</script>，第二部分是导数项，有什么用？</p>
<p>correct：<strong>Simultaneous</strong> update更新参数</p>
<script type="math/tex; mode=display">temp0:=\theta_0-\alpha\frac{\partial}{\partial\theta_0}J(\theta_0,\theta_1)</script><script type="math/tex; mode=display">temp1:=\theta_1-\alpha\frac{\delta}{\delta\theta_1}J(\theta_0,\theta_1)</script><script type="math/tex; mode=display">\theta_0:=temp0</script><script type="math/tex; mode=display">\theta_1:=temp1</script><p>上面满足<strong>同步更新</strong>，与下面的方式不同</p>
<p>Incorrect：</p>
<script type="math/tex; mode=display">temp0:=\theta_0-\alpha\frac{\partial}{\partial\theta_0}J(\theta_0,\theta_1)</script><script type="math/tex; mode=display">\theta_0:=temp0</script><script type="math/tex; mode=display">temp1:=\theta_1-\alpha\frac{\partial}{\partial\theta_1}J(\theta_0,\theta_1)$$（$$\theta_0$$更新后计算这个倒数项）

$$\theta_1:=temp1</script></blockquote>
<p>微妙处，对于更新方程，同时更新<script type="math/tex">\theta_0,\theta_1</script>，梯度下降<script type="math/tex">\to</script>同步更新</p>
<h2 id="2-6-梯度下降知识点总结"><a href="#2-6-梯度下降知识点总结" class="headerlink" title="2.6 梯度下降知识点总结"></a>2.6 梯度下降知识点总结</h2><p>梯度算法是做什么的以及梯度下降算法的更新过程有什么意义</p>
<p>上面梯度算法，两部分有什么用？以及为什么当把这两部分放在一起时，整个更新过程是有意义的？</p>
<p>考虑最小化函数只有一个参数的情形，<script type="math/tex">\underbrace{min}_{\theta_1} J(\theta_1)，\theta_1\in R</script>，画出一维的曲线（想象一个二次函数曲线，开口向上），选取一点（正斜率的那点），不断更新，<script type="math/tex">\theta_1=\theta_1-\alpha*\frac{d}{d\theta_1}J(\theta_1)，\alpha</script>永远是一个正数，又因为是正斜率，则<script type="math/tex">\frac{d}{d\theta_1}J(\theta_1)</script>为正数，<script type="math/tex">\theta_1</script>减去正数，其减小，则会向最小点方向移动，更接近最低点；同理我们取斜率为负的点，<script type="math/tex">\theta_1</script>会增大。那么研究更新规则<script type="math/tex">\alpha</script>，如果其太大或太小会出现什么情况，如果<script type="math/tex">\alpha</script>太小，需要很多步才能到达最低点；如果<script type="math/tex">\alpha</script>太大，那么梯度下降可能会越过最低点，甚至可能无法收敛或发散？离最低点越来越远，斜率变大，使<script type="math/tex">\theta_1</script>的值变化很大，所以无法收敛甚至发散。</p>
<p>如果<script type="math/tex">\theta_1</script>已经处在一个局部最优点，下一步梯度下降会怎样？假设将<script type="math/tex">\theta_1</script>初始化在局部最低点或最优处，局部最优处导数等于0，导数项为0，<script type="math/tex">\theta_1</script>将不再改变，梯度下降更新其实什么都没做，但这正是我们想要的，它使解始终保持在局部最优点。这也解释了，即使学习速率<script type="math/tex">\alpha</script>保持不变，梯度下降法也可以收敛到局部最低点的原因。</p>
<p>随着越接近最优处，导数越小接近0，<script type="math/tex">\theta_0</script>更新的幅度就会更小，所以梯度下降法会自动采用更小的幅度，这就是梯度下降的运行方式，所以没必要在另外减小<script type="math/tex">\alpha</script>。</p>
<p>回归本质，线性回归中的代价函数，平方代价函数，综合梯度下降函数</p>
<h2 id="2-7-线性回归的梯度下降"><a href="#2-7-线性回归的梯度下降" class="headerlink" title="2.7 线性回归的梯度下降"></a>2.7 线性回归的梯度下降</h2><p>将梯度下降和代价函数结合，得到线性回归算法，它可以用直线模型来拟合数据，2.2节线性假设和平方差代价函数。将梯度算法应用到最小化平方差代价函数，为了应用梯度下降法，写好<code>repeat until convergence</code>部分代码，关键步骤是那个导数项，弄清楚这个偏导数是什么，写出代价函数J，</p>
<p>写出<script type="math/tex">\frac{\partial}{\partial\theta_j}J(\theta_0,\theta_1)=\frac{\partial}{\partial\theta_j}\frac{1}{2M}\sum_{i=1}^M[h_\theta(x^{(i)})-y^{(i)}]^2=\frac{\partial}{\partial\theta_j}\frac{1}{2M}\sum_{i=1}^M[\theta_0+\theta_1x^{(i)}-y^{(i)}]^2</script></p>
<p>将假设的定义带入，在j等于0和J等于1的时候，两种情况的偏导数，简化，在训练集中求和</p>
<p>一：<script type="math/tex">j=0:\ \frac{\partial}{\partial\theta_0}J(\theta_0,\theta_1)=\frac{1}{M}\sum_{i=1}^M[h_\theta(x^{(i)})-y^{(i)}]</script></p>
<p>二：<script type="math/tex">j=1:\ \frac{\partial}{\partial\theta_1}J(\theta_0,\theta_1)=\frac{1}{M}\sum_{i=1}^M[h_\theta(x^{(i)})-y^{(i)}]*x^{(i)}</script></p>
<p>算出微分，即函数J的斜率，现将他们带回梯度下降算法，</p>
<blockquote>
<p>repeat until convergence（收敛）{</p>
<p>​        <script type="math/tex">\theta_0:=\theta_0-\alpha\frac{1}{m}\sum_{i=1}^m[h_\theta(x^{(i)})-y^{(i)}]</script></p>
<p>​        <script type="math/tex">\theta_1:=\theta_1-\alpha\frac{1}{m}\sum_{i=1}^m[h_\theta(x^{(i)})-y^{(i)}]*x^{(i)}</script></p>
<p>} 不断重复该过程直到收敛，<script type="math/tex">\theta_0和\theta_1</script>更新为原值减去<script type="math/tex">\alpha</script>乘后面的微分项，这就是线性回归算法。注意一些细节，才能同时更新<script type="math/tex">\theta_0和\theta_1</script>。梯度算法容易陷入局部最优，如图2，但线性回归的代价函数总是一个弓状函数，凸函数，这个函数没有局部最优解，只有一个全局最优解，总会收敛，</p>
</blockquote>
<p><img src="https://gitee.com/mysteryguest/ObjectStorage/raw/master/MachineLearing/convex_function.PNG" alt="图3" style="zoom:67%;" /></p>
<p>看假设函数<script type="math/tex">h_{\theta}(x)</script>和代价函数<script type="math/tex">J(\theta_0,\theta_1)</script>，通常在零点初始化，形象解释，初始化<script type="math/tex">\theta_0=900,\theta_1=-0.1,h(x)=-900-0.1x</script>，代价函数往下移（接近最优解），假设函数变化，越来越符合数据，Batch梯度下降，每一步下降，都遍历了整个训练集样本m，有的梯度下降算法，每次只关注小的子集。梯度下降适用于更大的数据集。</p>
<p>正规方程组法。</p>
<h2 id="3-1-矩阵和向量"><a href="#3-1-矩阵和向量" class="headerlink" title="3.1 矩阵和向量"></a>3.1 矩阵和向量</h2><p>矩阵，数组，矩阵的项，表示，快速访问</p>
<p>向量是一种特殊的矩阵，常用1作为开始下标，大写字母表示矩阵，小写表示数字</p>
<h2 id="3-2-加法和标量乘法"><a href="#3-2-加法和标量乘法" class="headerlink" title="3.2 加法和标量乘法"></a>3.2 加法和标量乘法</h2><p>矩阵加法，矩阵乘法（kA），矩阵除法（<script type="math/tex">A/k=\frac1kA</script>）</p>
<h2 id="3-3-矩阵向量乘法"><a href="#3-3-矩阵向量乘法" class="headerlink" title="3.3 矩阵向量乘法"></a>3.3 矩阵向量乘法</h2><p>类似于矩阵乘法，不过其中有一个矩阵是行向量或者列向量。</p>
<script type="math/tex; mode=display">h_\theta(x)=-40+0.25x$$，转化为矩阵的形式：

$$\begin{bmatrix}
1&2014\\
1&1416\\
1&1534\\1&852\\
\end{bmatrix} * \begin{bmatrix}
-40\\
0.25\\
\end{bmatrix}=\begin{bmatrix}
-40*1+0.25*2104\\
-40*1+0.25*1416\\
-40*1+0.25*1534\\-40*1+0.25*852\\
\end{bmatrix}</script><p>转化为左边的式子，可以使代码简洁高效，后面线性回归会用到</p>
<h2 id="3-4-矩阵乘法"><a href="#3-4-矩阵乘法" class="headerlink" title="3.4 矩阵乘法"></a>3.4 矩阵乘法</h2><p>矩阵乘法是如何解决<script type="math/tex">\theta_0</script>和<script type="math/tex">\theta_1</script>的问题，不需要用到梯度下降算法的迭代。</p>
<p>矩阵相乘（<script type="math/tex">A*[b_1,b_2]</script>），转化为（<script type="math/tex">[A*b_1,A*b_2]</script>）。</p>
<p>House sizes:                       Have 3 competing hypotheses:</p>
<script type="math/tex; mode=display">\begin{cases} 2104\\ 1416\\1534\\852 \end{cases}
\quad\quad\quad\quad\quad\quad\quad \left \{  \begin{array}{c} h_\theta(x)=-40+0.25x \\h_\theta(x)=200+0.1x \\h_\theta(x)=-150+0.4x \end{array} \right.</script><p>构造矩阵</p>
<script type="math/tex; mode=display">\begin{bmatrix}
1&2014\\
1&1416\\
1&1534\\1&852\\
\end{bmatrix} * \begin{bmatrix}
-40&200&-150\\
0.25&0.1&0.4\\
\end{bmatrix}=\begin{bmatrix}
480&410&692\\
314&342&416\\
344&353&464\\173&285&191\\
\end{bmatrix}</script><p>线性代数库，并行计算</p>
<h2 id="3-5-矩阵乘法特征"><a href="#3-5-矩阵乘法特征" class="headerlink" title="3.5 矩阵乘法特征"></a>3.5 矩阵乘法特征</h2><p>结合律，单位矩阵，是方阵，对角线元素为1。</p>
<p>For any matrix A, <script type="math/tex">A_{mn}*I_{nn}=I_{mm}*A_{mn}=A_{mn}</script></p>
<p>交换律只有当其中有一个矩阵为单位矩阵才满足，且另一个矩阵为方阵。</p>
<h2 id="3-6-逆和转置"><a href="#3-6-逆和转置" class="headerlink" title="3.6 逆和转置"></a>3.6 逆和转置</h2><p>If A is an m * m matrix, and if it has an inverse, <script type="math/tex">AA^{-1}=A^{-1}A=I</script></p>
<p>求解逆矩阵的库</p>
<h2 id="4-1-多功能"><a href="#4-1-多功能" class="headerlink" title="4.1 多功能"></a>4.1 多功能</h2><p>新的线性回归的版本，这种形式适用于多个变量，或者多特征量的情况，比如之前利用只房租面积来预测房价，现在有房租楼层、年龄等信息，多个特征量，<script type="math/tex">h(x)=\theta_0+\theta_1x+.....\theta_nx</script>，多元线性回归</p>
<h2 id="4-2-多元梯度下降法"><a href="#4-2-多元梯度下降法" class="headerlink" title="4.2 多元梯度下降法"></a>4.2 多元梯度下降法</h2><p>讨论如何设定该假设的参数，如何使用梯度下降算法来处理多元线性回归，</p>
<p>多元线性回归的假设形式：</p>
<blockquote>
<p>Hypothesis: <script type="math/tex">h_\theta(x)=\theta^Tx=\theta_0x_0+\theta_1x_1+\theta_2x_2.....\theta_nx_n</script></p>
<p>Parameters: <script type="math/tex">\theta_0,\theta_1,...,\theta_n</script></p>
<p>Cost function: <script type="math/tex">J(\theta_0,\theta_1,...,\theta_n)=\frac{1}{2m}\sum_{i=1}^m[h_\theta(x^{(i)})-y^{(i)}]^2</script></p>
<p>其中已经按惯例是<script type="math/tex">x_0=1</script>，不将其看作n个独立的参数，而是考虑把这些参数看作一个n+1维的<script type="math/tex">\theta</script>向量</p>
<p>Gradient descent：（以下面的方式不断更新<script type="math/tex">\theta_j</script>）</p>
<p>Repeat {</p>
<p>​        <script type="math/tex">\theta_j:=\theta_j-\alpha\frac{\partial}{\partial\theta_j}J(\theta_0,...,\theta_n)</script></p>
<p>} (simultaneously update for every j=0,…,n)</p>
</blockquote>
<p>将此模型的参数看作一个向量，代价函数通过误差项的平方和来给定，但又不把代价函数看作这n+1个数的函数，使用更通用的方式把J写成参数<script type="math/tex">\theta</script>这个向量的函数，这里<script type="math/tex">\theta</script>是一个向量，</p>
<blockquote>
<p>Gradient descent：</p>
<p>Previously(n=1)：n=1时的梯度下降算法</p>
<p>Repeat {</p>
<p>​        <script type="math/tex">\theta_0:=\theta_0-\alpha\underbrace{\frac1m(h_\theta(x^{(i)})-y^{(i)})}_{\frac{\partial}{\partial\theta_0}J(\theta)}</script></p>
<p>​        <script type="math/tex">\theta_1:=\theta_1-\alpha\frac1m\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x^{(i)}</script></p>
<p>​        (simultaneously update <script type="math/tex">\theta_0,\theta_1</script>)</p>
<p>}</p>
<p>两个独立的更新规则，分别对应参数<script type="math/tex">\theta_0,\theta_1</script>，这是之前的，看现在的</p>
<p>New algorithm<script type="math/tex">(n\ge1)</script>：</p>
<p>Repeat {</p>
<p>​        <script type="math/tex">\theta_j:=\theta_j-\alpha\frac1m\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}</script></p>
<p>​        (simultaneously update <script type="math/tex">\theta_j</script> for j=0,…,n)</p>
<p>}</p>
<p>用于多元函数线性回归的梯度下降算法</p>
</blockquote>
<p>相似的，下面，考虑超过两个特征值的情况，因此，有3条更新规则来更新参数<script type="math/tex">\theta_0,\theta_1,\theta_2</script>：</p>
<blockquote>
<ul>
<li><script type="math/tex; mode=display">\theta_0:=\theta_0-\alpha\frac1m\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_0^{(i)}</script></li>
<li><script type="math/tex; mode=display">\theta_1:=\theta_1-\alpha\frac1m\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_1^{(i)}</script></li>
<li><script type="math/tex; mode=display">\theta_2:=\theta_2-\alpha\frac1m\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_2^{(i)}</script></li>
</ul>
<p>和之前两个参数的梯度算法是等效的</p>
</blockquote>
<p>这样就得到了可行的线性回归模型</p>
<h2 id="4-3-多元梯度下降法演练"><a href="#4-3-多元梯度下降法演练" class="headerlink" title="4.3 多元梯度下降法演练"></a>4.3 多元梯度下降法演练</h2><h3 id="4-3-1-特征缩放"><a href="#4-3-1-特征缩放" class="headerlink" title="4.3.1 特征缩放"></a>4.3.1 特征缩放</h3><p>有个机器学习问题，这个问题有多个特征，</p>
<blockquote>
<p>Feature Scaling</p>
<p>Idea: Make sure features are on a similar scale.（如果能确保这些特征都处在一个相近的范围），这样梯度下降法就能更快地收敛</p>
<p>E.g. <script type="math/tex">x_1=size(0-2000feet^2)</script></p>
<p>​       <script type="math/tex">x_2=number\ of\ bedrooms(1-5)</script></p>
<p>假设有个模型由两个参数，<script type="math/tex">x_1</script>表示房屋面积，<script type="math/tex">x_2</script>表示卧室数量，画出等值线图，是非常高大细长的椭圆形，构成了代价函数<script type="math/tex">J(\theta)</script>的等值线。</p>
</blockquote>
<p><img src="https://gitee.com/mysteryguest/ObjectStorage/raw/master/Thesis/j_func.jpg" style="zoom:67%;" /></p>
<p>如果在这种代价函数上运行梯度下降的话，最终可能需要花很长一段时间，并且可能会来回波动，然后会经过很长时间，最终才收敛到全局最小值。</p>
<p>如果这些等值线更夸张一些，椭圆画的更细更长，结果就是梯度下降更缓慢，反复来回震荡，才找到一条通往全局最小值的路。这种情况下，一种有效的方法是进行<strong>特征缩放</strong>。</p>
<p>具体来说，如果把特征<script type="math/tex">x_1</script>定义为房子的面积大小除以2000，并且把<script type="math/tex">x_2</script>定义为卧室的数量除以5，那么代价函数<script type="math/tex">J(\theta)</script>的等值线，就会变得偏移没那么严重，就会看起来更圆一些了，如果在这样的函数上执行梯度下降的话，就会找到一条更直接的路径，通向全局最小值，而不像上面那样，沿着一条复杂得多的路径。</p>
<p>因此，通过这些特征缩放，它们的值的范围变得相近，两个特征值都在0和1之间，这样得到的梯度下降算法就会更快地收敛。</p>
<p>更一般地，执行特征缩放时，将特征的取值约束到-1到+1的范围内，具体来说，特征<script type="math/tex">x_0</script>总是等于1，因此，这已经是在这个范围内（[-1, +1]），但对其他的特征，可能需要通过除以不同的数，来使其处于这个范围内。</p>
<p>但其实，<script type="math/tex">\pm1</script>不重要，如果特征<script type="math/tex">x_1\in[0,3]</script>，另一个特征<script type="math/tex">x_2\in[-2,0.5]</script>，这些范围其实非常接近<script type="math/tex">[-1,+1]</script>，其实都可以。但如果有个特征<script type="math/tex">x_4\in[-100,+100]</script>，这个范围和<script type="math/tex">[-1,+1]</script>有很大不同了，或者<script type="math/tex">x_4\in[-0.00001,+0.000001]</script>那么这同样是一个和<script type="math/tex">[-1,+1]</script>不同的范围。如何判断数据的不同，其实因人而异，比如以3为界。</p>
<p>只要范围相差不大，梯度下降算法就可以正常工作。除了将特征值除以最大值以外，在特征值缩放中，有时候也会进行一个称为均值归一化的工作，</p>
<blockquote>
<p>Mean normalization</p>
<p>Replace <script type="math/tex">x_i</script> with <script type="math/tex">x_i-\mu_i</script> to make features have approximately zero mean(Do not apply to <script type="math/tex">x_0=1</script>).</p>
<p>E.g. <script type="math/tex">x_1=\frac{size-1000}{2000}</script>（1000是2000的平均值）</p>
<p>​    <script type="math/tex">x_2=\frac{\#bedrooms-2}{5}</script></p>
<p>​    <script type="math/tex">-0.5\leq x_1\leq0.5,-0.5\leq x_2\leq0.5</script></p>
</blockquote>
<p>就是用<script type="math/tex">x_i-\mu_i</script>来替换特征<script type="math/tex">x_i</script>，让特征值具有为0的平均值，不需要将这一步运用到<script type="math/tex">x_0</script>中，因为其始终等于1，不可能有为0的平均值。</p>
<p>还可以这样代替：<script type="math/tex">x_1\leftarrow \frac{x_1-\mu_1}{s_1}</script>，定义<script type="math/tex">\mu_1</script>是训练集中特征<script type="math/tex">x_1</script>的平均值，<script type="math/tex">s_1</script>是该特征值的范围，这里的范围是指最大值减去最小值，同理其他特征值。</p>
<p>其实特征缩放并不精确，但能够运行得更快一点而已，迭代次数更少。</p>
<h3 id="4-3-2-学习率"><a href="#4-3-2-学习率" class="headerlink" title="4.3.2 学习率"></a>4.3.2 学习率</h3><p>将具体讨论学习率<script type="math/tex">\alpha</script>，即梯度算法的更新规则</p>
<blockquote>
<p>Gradient descent</p>
<p>​        <script type="math/tex">\theta_j:=\theta_j-\alpha\frac{\partial}{\partial\theta_j}J(\theta)</script></p>
<ul>
<li>“Debugging”: How to make sure gradient descent is workinng correctly.</li>
<li>How to choose learing rate <script type="math/tex">\alpha</script>.</li>
</ul>
</blockquote>
<p>调试，小技巧来确保梯度下降是正常工作的，并解释如何选择学习率<script type="math/tex">\alpha</script>。</p>
<p>在梯度下降算法工作时，给出代价函数<script type="math/tex">J(\theta)</script>的值，x轴表示的是梯度下降算法的迭代次数（与之前不同，之前x轴是<script type="math/tex">\theta</script>）。代价函数随迭代步数增加的变化曲线。</p>
<p><img src="https://gitee.com/mysteryguest/ObjectStorage/raw/master/Thesis/j_diedai.jpg" style="zoom:67%;" /></p>
<p>A点的含义：运行100次的梯度下降迭代，无论100步迭代后，得到什么<script type="math/tex">\theta</script>值，在100次迭代后得到某个<script type="math/tex">\theta</script>值，对于这个<script type="math/tex">\theta</script>值，将评估代价函数<script type="math/tex">J(\theta)</script>，A点的垂直高度就代表：梯度下降算法100步迭代之后得到的<script type="math/tex">\theta</script>算出的<script type="math/tex">J(\theta)</script>值。后面同理。</p>
<p>这条曲线的用处在于，它可以告诉你，在后面迭代次数增加时，看起来<script type="math/tex">J(\theta)</script>并没有下降多少，到达400后，已经很平坦了，梯度下降算法差不多已经收敛了，因为代价函数没有再继续下降了，所以通过这条曲线可以帮助判断，梯度下降算法是否已经收敛。</p>
<p>不同模型的梯度下降算法收敛时迭代次数可能回想相差很大，有些模型梯度下降算法只需要30步迭代就可以达到收敛，然而换一个问题就需要3000步迭代。实际上我们很难判断一个问题的梯度下降算法需要多少步迭代才能收敛，</p>
<p>通常通过看这种曲线来判断，梯度下降算法是否已经收敛，也可以进行一些自动的收敛测试，让另一种算法来判断梯度下降算法是否已经收敛，自动收敛测试。</p>
<p>例：如果代价函数<script type="math/tex">J(\theta)</script>一步迭代后的下降小于一个很小的值<script type="math/tex">\epsilon</script>，这个测试就判断函数已收敛，<script type="math/tex">\epsilon</script>可以是<script type="math/tex">10^{-3}</script>，不过通常要选择一个合适的阈值<script type="math/tex">\epsilon</script>是相当困难的。因此，为了检测梯度下降算法是否收敛，通常还是通过上面的曲线，这种曲线还可以显示出或警告算法没有正常工作。</p>
<p>如果得到的代价函数随迭代步数增加的变化曲线逐渐上升的曲线，即代价函数随迭代次数增多变得更大（造成这种情况的原因比如，目标函数是二次函数曲线），不趋于稳定，而这样的曲线图通常意味着应该使用较小的学习率<script type="math/tex">\alpha</script>（梯度算法会不断越过最小值，得到代价函数越来越大）。</p>
<p>已证明，学习率足够小，那么每次迭代后代价函都会下降；但不易过小。</p>
<p>每隔10倍（3倍）取一个<script type="math/tex">\alpha</script>值，然后对于这些不同的<script type="math/tex">\alpha</script>值，绘制<script type="math/tex">J(\theta)</script>随迭代步数变化的曲线，然后选择使得<script type="math/tex">J(\theta)</script>快速下降的一个<script type="math/tex">\alpha</script>值，如何找到合适的学习率。</p>
<p>找到一个太小的值，再找到另一个太大的值，然后取最大可能值，或者比最大值略小一些的，比较合理的值。</p>
<h2 id="4-4-特征和多项式回归"><a href="#4-4-特征和多项式回归" class="headerlink" title="4.4 特征和多项式回归"></a>4.4 特征和多项式回归</h2><p>一些可供选择的特征，以及如何得到不同的学习算法，当选择了合适的特征后，这些算法往往是非常有效的，</p>
<p>多项式回归，使得能够使用线性回归的方法来拟合非常复杂的函数，甚至是非线性函数，以预测房价为例，</p>
<p>假设有两个特征，分别是房子临街的宽度（<script type="math/tex">x_1</script>）和垂直宽度（<script type="math/tex">x_2</script>），在用线性回归时，不一定非要用<script type="math/tex">x_1</script>和<script type="math/tex">x_2</script>作为特征，可以创造新的特征，因此，如果要预测房子的价格，会做的是确认真正能够决定房子大小的是拥有的土地的大小。新的特征即临街宽度与纵深的乘积，就是拥有土地的面积，于是将这个乘积作为假设，只用一个特征，就是土地的面积。具体问题取决视情况而定，有时通过定义新的特征，可以得到更好的模型。</p>
<p>与选择特征的想法，密切相关的一个概念，被称为多项式回归，例如，现有这样一个住房价格的数据集，可能会有多个不同的模拟用于拟合，选择之一是像这样的二次模型，直线似乎并不能很好地拟合这些数据，因此，会想到，用这样的二次模型去拟合，会考虑到价格可能是一个二次函数，得到曲线一样的拟合结果，二次模型合理吗？因为一个二次模型最终会（先上升）降下来，但事实是房价会随着面积增加而下降？因此，也许会选择一个不同的多项式模型，并转而选择使用一个三次函数，用其进行拟合，不会最后下降。那么如何将模型与数据进行拟合呢？</p>
<p>使用多元线性回归的方法，对算法做一个简单的修改来实现它，按照之前的假设，</p>
<p>即<script type="math/tex">h_\theta(x)=\theta_0+\theta_1(size)+\theta_2(size)^2+\theta_3(size)^3,x_1=(size),x_2=(size)^2,x_3=(size)^3</script>特征缩放变得重要，因为房子大小在1到1000之间，那么立方就会到<script type="math/tex">10^9</script>，因此这三个特征有很大不同，这样才能将值的范围变得具有可比性，</p>
<p>除了3次模型不会下降，还有<script type="math/tex">h_\theta(x)=\theta_0+\theta_1(size)+\theta_2\sqrt{(size)}</script>凭借对数据的形状和函数的了解，</p>
<h2 id="4-5-正规方程（区别于迭代方法的直接解法）"><a href="#4-5-正规方程（区别于迭代方法的直接解法）" class="headerlink" title="4.5 正规方程（区别于迭代方法的直接解法）"></a>4.5 正规方程（区别于迭代方法的直接解法）</h2><p>目前为止，一直使用线性回归方法，梯度下降法，</p>
<p>Normal equation: Method to solve for <script type="math/tex">\theta</script> analytically.（不用迭代，一次性求解<script type="math/tex">\theta</script>的最优值）</p>
<p>优缺点及何时使用这个算法</p>
<blockquote>
<p>Intuition: if 1D (<script type="math/tex">\theta\in R</script>)</p>
<p>​    <script type="math/tex">J(\theta)=a\theta^2+b\theta+c</script></p>
<p>这是个二次函数，怎么求极值，倒数置0，</p>
<script type="math/tex; mode=display">\theta\in R^{n+1}\quad J(\theta_0,\theta_1,...,\theta_m)=\frac{1}{2m}\sum_{i=1}^m[h_\theta(x^{(i)})-y^{(i)}]^2 \quad \frac{\partial}{\partial\theta_j}J(\theta)=...=0 (for every j)</script><p>Solve for <script type="math/tex">\theta_0,\theta_1,...,\theta_n</script></p>
<p>这里实际问题中<script type="math/tex">\theta</script>是n+1维向量，代价函数是关于这个向量的函数，那么现在如何最小化这个函数呢？是对向量中每个参数<script type="math/tex">\theta_j</script>对代价函数求偏导，然后把这些偏导数全部置0。</p>
<p>但是遍历所有偏微分，麻烦，</p>
</blockquote>
<p>Example：m=4，4个训练样本，如何使用正规方程方法，假设这4个样本是所有数据，加一列对应额外特征变量的<script type="math/tex">x_0</script>，</p>
<div class="table-container">
<table>
<thead>
<tr>
<th><script type="math/tex">x_0</script></th>
<th>Size<script type="math/tex">(feet^2) \ x_1</script></th>
<th>Number of bedrooms <script type="math/tex">x_2</script></th>
<th>Number of floors <script type="math/tex">x_3</script></th>
<th>Age of home (years) <script type="math/tex">x_4</script></th>
<th>Price($1000)</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>2104</td>
<td>5</td>
<td>1</td>
<td>45</td>
<td>460</td>
</tr>
<tr>
<td>1</td>
<td>1416</td>
<td>3</td>
<td>2</td>
<td>40</td>
<td>232</td>
</tr>
<tr>
<td>1</td>
<td>1534</td>
<td>3</td>
<td>2</td>
<td>30</td>
<td>315</td>
</tr>
<tr>
<td>1</td>
<td>852</td>
<td>2</td>
<td>1</td>
<td>36</td>
<td>178</td>
</tr>
</tbody>
</table>
</div>
<p>构建一个矩阵X，<script type="math/tex">X=\begin{bmatrix}
1&2014&5&1&45\\
1&1416&3&2&40\\
1&1534&3&2&30\\1&852&2&1&36\\
\end{bmatrix}\quad \quad y=\begin{bmatrix}460\\232\\315\\178 \end{bmatrix}</script></p>
<p>X是一个<script type="math/tex">m*(n+1)</script>维矩阵，y是一个m维向量，</p>
<p>使用<script type="math/tex">\theta=(X^TX)^{-1}X^Ty</script>，就能得到使代价函数最小化的<script type="math/tex">\theta</script>。</p>
<blockquote>
<p>m examples <script type="math/tex">(x^{(1)},y^{(1)}),...,(x^{(m), y^{m}})</script>; n features.</p>
<p>所以每个训练样本可能是：<script type="math/tex">x^{i}=\begin{bmatrix}x_0^{(i)}\\x_1^{(i)}\\x_2^{(i)}\\...\\x_n^{(i)} \end{bmatrix}\in R^{n+1}</script></p>
<p>构建矩阵的方法，设计矩阵，用上面向量的转置作为矩阵X的行</p>
<p>矩阵X是一个<script type="math/tex">m*(n+1)</script>维矩阵，<script type="math/tex">X=\begin{bmatrix}
(x^{(1)})^T\\
(x^{(1)})^T\\
...\\(x^{(m)})^T\\
\end{bmatrix}</script></p>
</blockquote>
<p>使用正规方程方法就不需要特征缩放，</p>
<p>何时应该使用梯度下降法，何时使用正规方程法（优点和缺点）</p>
<blockquote>
<p>m <strong>training examples</strong>, n <strong>feature</strong>.</p>
<p><strong>Gradient Descent</strong></p>
<ul>
<li>缺点<ul>
<li>Need to choose <script type="math/tex">\alpha</script>.</li>
<li>Needs many iterations.更多的迭代</li>
</ul>
</li>
<li>优点<ul>
<li>Works well even when n is large.</li>
</ul>
</li>
</ul>
<p><strong>Normal Equation</strong></p>
<ul>
<li>优点<ul>
<li>No need to choose <script type="math/tex">\alpha</script>.（也不需要检测收敛性）</li>
<li>Don’t need to iterate.</li>
</ul>
</li>
<li>缺点<ul>
<li>Need to compute <script type="math/tex">(X^TX)^{-1}</script>，n*n的矩阵，计算量大，<script type="math/tex">O(n^3)</script></li>
<li>Slow if n is very large</li>
</ul>
</li>
</ul>
<p>根据习惯，n大于1万，就使用梯度算法，</p>
</blockquote>
<h2 id="4-6-正规方程在矩阵不可逆情况下的解决方法"><a href="#4-6-正规方程在矩阵不可逆情况下的解决方法" class="headerlink" title="4.6 正规方程在矩阵不可逆情况下的解决方法"></a>4.6 正规方程在矩阵不可逆情况下的解决方法</h2><h2 id="4-7-导师的编程小技巧"><a href="#4-7-导师的编程小技巧" class="headerlink" title="4.7 导师的编程小技巧"></a>4.7 导师的编程小技巧</h2></div><div class="article-tags size-small mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/%E7%AE%97%E6%B3%95/">算法</a></div><div class="a2a_kit a2a_kit_size_32 a2a_default_style"><a class="a2a_dd" href="https://www.addtoany.com/share"></a><a class="a2a_button_facebook"></a><a class="a2a_button_twitter"></a><a class="a2a_button_telegram"></a><a class="a2a_button_whatsapp"></a><a class="a2a_button_reddit"></a></div><script src="https://static.addtoany.com/menu/page.js" defer></script></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered">喜欢这篇文章？打赏一下作者吧</h3><div class="buttons is-centered"><a class="button is-info donate"><span class="icon is-small"><i class="fab fa-alipay"></i></span><span>支付宝</span><span class="qrcode"><img src="/donate/alipay.jpg" alt="支付宝"></span></a><a class="button is-success donate"><span class="icon is-small"><i class="fab fa-weixin"></i></span><span>微信</span><span class="qrcode"><img src="/donate/wechatpay.png" alt="微信"></span></a></div></div></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/Machine-Learing/32df0da.html"><i class="level-item fas fa-chevron-left"></i><span class="level-item">起步</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/GitHub/6eb7acd3.html"><span class="level-item">常用</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">评论</h3><div class="content" id="valine-thread"></div><script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script><script src="https://cdn.jsdelivr.net/npm/valine@1.4.14/dist/Valine.min.js"></script><script>new Valine({
            el: '#valine-thread' ,
            appId: "vucxC0bltE95lsPGxGXlUJBf-gzGzoHsz",
            appKey: "YgRn5inwLxs6B4VgyLp4zvti",
            placeholder: "ヾﾉ≧∀≦)o快来评论一下吧!",
            avatar: "mm",
            
            meta: ["nick","mail","link"],
            pageSize: 10,
            lang: "zh-CN",
            
            highlight: true,
            
            
            
            
            
            requiredFields: [],
        });</script></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1 is-sticky"><div class="card widget"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/shadow.jpg" alt="肖梦杰"></figure><p class="title is-size-4 is-block line-height-inherit">肖梦杰</p><p class="is-size-6 is-block">MysticalGuest</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Xiangyang City, Hubei Province, China</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/archives"><p class="title">65</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/categories"><p class="title">17</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/tags"><p class="title">17</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/MysticalGuest" target="_blank" rel="noopener">关注我</a></div><div class="level is-mobile"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/MysticalGuest"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="QQ" href="https://user.qzone.qq.com/1317148109/main"><i class="fab fa-qq"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Weibo" href="https://weibo.com/MysticalGuest"><i class="fab fa-weibo"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Email" href="mailto:etherealsymbol@gmail.com"><i class="fa fa-envelope"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/atom.xml"><i class="fas fa-rss"></i></a></div></div></div><div class="card widget" id="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">目录</h3><ul class="menu-list"><li><a class="is-flex" href="#第一章-绪论"><span class="mr-2">1</span><span>第一章 绪论</span></a><ul class="menu-list"><li><a class="is-flex" href="#1-1-什么是机器学习"><span class="mr-2">1.1</span><span>1.1 什么是机器学习</span></a></li><li><a class="is-flex" href="#1-2-机器学习的应用实例"><span class="mr-2">1.2</span><span>1.2 机器学习的应用实例</span></a><ul class="menu-list"><li><a class="is-flex" href="#1-2-1-学习关联性"><span class="mr-2">1.2.1</span><span>1.2.1 学习关联性</span></a></li><li><a class="is-flex" href="#1-2-2-分类"><span class="mr-2">1.2.2</span><span>1.2.2 分类</span></a></li><li><a class="is-flex" href="#1-2-3-回归"><span class="mr-2">1.2.3</span><span>1.2.3 回归</span></a></li><li><a class="is-flex" href="#1-2-4-非监督学习"><span class="mr-2">1.2.4</span><span>1.2.4 非监督学习</span></a></li><li><a class="is-flex" href="#1-2-5-增强学习"><span class="mr-2">1.2.5</span><span>1.2.5 增强学习</span></a></li></ul></li></ul></li><li><a class="is-flex" href="#第二章-监督学习"><span class="mr-2">2</span><span>第二章 监督学习</span></a><ul class="menu-list"><li><a class="is-flex" href="#2-1-由实例学习类"><span class="mr-2">2.1</span><span>2.1 由实例学习类</span></a></li></ul></li><li><a class="is-flex" href="#上海交大张志华机器学习"><span class="mr-2">3</span><span>上海交大张志华机器学习</span></a></li><li><a class="is-flex" href="#吴恩达机器学习"><span class="mr-2">4</span><span>吴恩达机器学习</span></a><ul class="menu-list"><li><a class="is-flex" href="#1-3-监督学习"><span class="mr-2">4.1</span><span>1.3 监督学习</span></a></li><li><a class="is-flex" href="#1-4-无监督学习"><span class="mr-2">4.2</span><span>1.4 无监督学习</span></a></li><li><a class="is-flex" href="#2-1-模型描述"><span class="mr-2">4.3</span><span>2.1 模型描述</span></a></li><li><a class="is-flex" href="#2-2-代价函数"><span class="mr-2">4.4</span><span>2.2 代价函数</span></a><ul class="menu-list"><li><a class="is-flex" href="#零"><span class="mr-2">4.4.1</span><span>零</span></a></li><li><a class="is-flex" href="#一"><span class="mr-2">4.4.2</span><span>一</span></a></li><li><a class="is-flex" href="#二"><span class="mr-2">4.4.3</span><span>二</span></a></li></ul></li><li><a class="is-flex" href="#2-5-梯度下降"><span class="mr-2">4.5</span><span>2.5 梯度下降</span></a></li><li><a class="is-flex" href="#2-6-梯度下降知识点总结"><span class="mr-2">4.6</span><span>2.6 梯度下降知识点总结</span></a></li><li><a class="is-flex" href="#2-7-线性回归的梯度下降"><span class="mr-2">4.7</span><span>2.7 线性回归的梯度下降</span></a></li><li><a class="is-flex" href="#3-1-矩阵和向量"><span class="mr-2">4.8</span><span>3.1 矩阵和向量</span></a></li><li><a class="is-flex" href="#3-2-加法和标量乘法"><span class="mr-2">4.9</span><span>3.2 加法和标量乘法</span></a></li><li><a class="is-flex" href="#3-3-矩阵向量乘法"><span class="mr-2">4.10</span><span>3.3 矩阵向量乘法</span></a></li><li><a class="is-flex" href="#3-4-矩阵乘法"><span class="mr-2">4.11</span><span>3.4 矩阵乘法</span></a></li><li><a class="is-flex" href="#3-5-矩阵乘法特征"><span class="mr-2">4.12</span><span>3.5 矩阵乘法特征</span></a></li><li><a class="is-flex" href="#3-6-逆和转置"><span class="mr-2">4.13</span><span>3.6 逆和转置</span></a></li><li><a class="is-flex" href="#4-1-多功能"><span class="mr-2">4.14</span><span>4.1 多功能</span></a></li><li><a class="is-flex" href="#4-2-多元梯度下降法"><span class="mr-2">4.15</span><span>4.2 多元梯度下降法</span></a></li><li><a class="is-flex" href="#4-3-多元梯度下降法演练"><span class="mr-2">4.16</span><span>4.3 多元梯度下降法演练</span></a><ul class="menu-list"><li><a class="is-flex" href="#4-3-1-特征缩放"><span class="mr-2">4.16.1</span><span>4.3.1 特征缩放</span></a></li><li><a class="is-flex" href="#4-3-2-学习率"><span class="mr-2">4.16.2</span><span>4.3.2 学习率</span></a></li></ul></li><li><a class="is-flex" href="#4-4-特征和多项式回归"><span class="mr-2">4.17</span><span>4.4 特征和多项式回归</span></a></li><li><a class="is-flex" href="#4-5-正规方程（区别于迭代方法的直接解法）"><span class="mr-2">4.18</span><span>4.5 正规方程（区别于迭代方法的直接解法）</span></a></li><li><a class="is-flex" href="#4-6-正规方程在矩阵不可逆情况下的解决方法"><span class="mr-2">4.19</span><span>4.6 正规方程在矩阵不可逆情况下的解决方法</span></a></li><li><a class="is-flex" href="#4-7-导师的编程小技巧"><span class="mr-2">4.20</span><span>4.7 导师的编程小技巧</span></a></li></ul></li></ul></div></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">链接</h3><ul class="menu-list"><li><a class="level is-mobile is-mobile" href="https://hexo.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Hexo</span></span><span class="level-right"><span class="level-item tag">hexo.io</span></span></a></li><li><a class="level is-mobile is-mobile" href="https://bulma.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Bulma</span></span><span class="level-right"><span class="level-item tag">bulma.io</span></span></a></li><li><a class="level is-mobile is-mobile" href="https://spring.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Spring</span></span><span class="level-right"><span class="level-item tag">spring.io</span></span></a></li><li><a class="level is-mobile is-mobile" href="https://redis.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Redis</span></span><span class="level-right"><span class="level-item tag">redis.io</span></span></a></li><li><a class="level is-mobile is-mobile" href="https://docker.com" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Docker</span></span><span class="level-right"><span class="level-item tag">docker.com</span></span></a></li></ul></div></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">分类</h3><ul class="menu-list"><li><a class="level is-mobile is-marginless" href="/categories/Axios/"><span class="level-start"><span class="level-item">Axios</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/C/"><span class="level-start"><span class="level-item">C</span></span><span class="level-end"><span class="level-item tag">14</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/CSharp/"><span class="level-start"><span class="level-item">CSharp</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/ElasticSearch/"><span class="level-start"><span class="level-item">ElasticSearch</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/Electron-Vue/"><span class="level-start"><span class="level-item">Electron-Vue</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/Element-UI/"><span class="level-start"><span class="level-item">Element-UI</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/GitHub/"><span class="level-start"><span class="level-item">GitHub</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/Hexo/"><span class="level-start"><span class="level-item">Hexo</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/Interview/"><span class="level-start"><span class="level-item">Interview</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/Java/"><span class="level-start"><span class="level-item">Java</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/Machine-Learing/"><span class="level-start"><span class="level-item">Machine-Learing</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/OS/"><span class="level-start"><span class="level-item">OS</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/Redis/"><span class="level-start"><span class="level-item">Redis</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/Spring/"><span class="level-start"><span class="level-item">Spring</span></span><span class="level-end"><span class="level-item tag">12</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/Study/"><span class="level-start"><span class="level-item">Study</span></span><span class="level-end"><span class="level-item tag">13</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/Thesis/"><span class="level-start"><span class="level-item">Thesis</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/TypeScript/"><span class="level-start"><span class="level-item">TypeScript</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">标签</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/English/"><span class="tag">English</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E4%B8%BB%E9%A2%98/"><span class="tag">主题</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%89%8D%E7%AB%AF/"><span class="tag">前端</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%8E%9F%E7%90%86/"><span class="tag">原理</span><span class="tag is-grey-lightest">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%9F%BA%E7%B1%BB/"><span class="tag">基类</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%A4%8D%E8%AF%95/"><span class="tag">复试</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E/"><span class="tag">搜索引擎</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"><span class="tag">数据库</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%A0%A1%E6%8B%9B/"><span class="tag">校招</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%A1%86%E6%9E%B6/"><span class="tag">框架</span><span class="tag is-grey-lightest">12</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%A7%AF%E7%B4%AF/"><span class="tag">积累</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%AE%97%E6%B3%95/"><span class="tag">算法</span><span class="tag is-grey-lightest">17</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%80%83%E7%A0%94/"><span class="tag">考研</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/"><span class="tag">计算机基础</span><span class="tag is-grey-lightest">7</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%AF%BE%E7%A8%8B/"><span class="tag">课程</span><span class="tag is-grey-lightest">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E9%85%8D%E7%BD%AE/"><span class="tag">配置</span><span class="tag is-grey-lightest">7</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E9%9D%A2%E8%AF%95%E9%A2%98%E9%9B%86/"><span class="tag">面试题集</span><span class="tag is-grey-lightest">2</span></a></div></div></div></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">订阅更新</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" value="" name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button is-primary" type="submit" value="订阅"></div></div></form></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/mylogo.svg" alt="Longing(MysteryGuest的博客)" height="28"></a><p class="size-small"><span>&copy; 2021 MysticalGuest</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener nofollow">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener nofollow">Icarus</a><br><span id="busuanzi_container_site_uv">共<span id="busuanzi_value_site_uv">0</span>个访客</span><br><span id="busuanzi_container_site_pv">本站总访问量<span id="busuanzi_value_site_pv">0</span>次</span><br><span id="timeDate">载入天数...</span><span id="times">载入时分秒...</span><span><script>	var now = new Date();	function createtime() {		var grt= new Date("4/2/2020 08:00:00");		now.setTime(now.getTime()+250);		days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days); 		hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours); 		if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum); 		mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;} 		seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum); 		snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;} 		document.getElementById("timeDate").innerHTML = "本站已安全运行 "+dnum+" 天 "; 		document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒";	}	setInterval("createtime()",250);</script></span></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="GitHub" href="https://github.com/MysticalGuest"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script>moment.locale("zh-CN");</script><script>var IcarusThemeSettings = {
            site: {
                url: 'https://mysticalguest.github.io',
                external_link: {"enable":true,"exclude":[]}
            },
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><script src="/js/click.js"></script><script src="/js/anime.min.js"></script><canvas class="fireworks" style="position: fixed;left: 0;top: 0;z-index: 1; pointer-events: none;"></canvas><script src="/js/snow.js"></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>